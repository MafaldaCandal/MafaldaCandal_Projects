Artificial intelligence (AI) has become a transformative driver in the workforce, automating simple and repetitive tasks, but it has gone further, tackling more complex challenges like recruitment. Automated hiring systems made great promises of efficiency and impartiality by using AI algorithms to screen, rank and select candidates. However, numerous cases reveal that these technologies can inherit and even amplify existing bias, leading to discriminatory hiring practices (Bogen, 2019). This paper dives into the nature of algorithmic bias in hiring, its classification, and its consequences, focusing on Amazon's failed AI recruitment tool as a case study.
In 2014, Amazon implemented an AI hiring tool designed to streamline its vast recruitment process by automatically ranking job applicants. The AI model quickly revealed significant gender bias, penalising words like "women’s," as in "women’s chess club captain," and downgraded applicants from all-women’s colleges. Additionally, it favoured resumes that contained expressions more commonly used by men, reinforcing the preference for male candidates. Following multiple fruitless attempts to rectify these biases, Amazon completely abandoned the project in 2017. The company concluded that ensuring true neutrality in automated hiring was not feasible at that time (Dastin, 2018). The source of the bias in this project will be explored below alongside the concepts of section and historical bias.
Selection bias occurs when the data used to train the AI model does not fairly represent the problem that it was designed to solve. This bias is often subtle, but it can have real-world implications. For instance, a facial recognition system developed in a predominantly white country, can lead to misidentifications in security settings (Johnson & Johnson, 2023). In Amazon’s case, the algorithm was trained using predominantly male applicant’s resumes from the previous decade (Dastin, 2018), but I would not classify this as selection bias as that is in fact all the data available. Unlike selection bias, which results from sampling issues, historic bias does reflect deeper systemic inequalities that AI may inherit and reinforce. For example, COMPAS recidivism algorithm, which disproportionately assigned higher risk scores to black defendants compared to white defendants, even when controlling for similar past offenses (Angwin et al., 2016). And returning to Amazon's hiring tool, historic bias seems to play a key role, as the tech industry has long been a male-dominant field (Dastin, 2018). This imbalance is reflected in the hiring data used to train the AI, which learns to perpetuate these patterns, making it harder for female candidates to be recommended.
The failed case of Amazon’s AI hiring tool highlights the broader dangers of algorithmic bias in recruitment. If unaddressed, automated hiring systems’ discrimination can perpetuate unfair hiring practices, negatively impacting underrepresented groups. Moreover, it can even have legal and reputation repercussions as discriminatory hiring practices violate employment laws. And lastly, it threatens workplace diversity and limits innovation. To prevent an aggravation of this danger, organizations must take proactive steps to reduce AI bias in recruitment, including ensuring diverse and balanced training data, and conducting algorithmic audits and increasing transparency to allow for biases to be detected and corrected. And, for now, it is advisable that human supervision complements AI-driven decisions to reduce automated discrimination (IBM, 2023).
On a final note, while there is much promise for AI’s use in streamlining hiring processes, cases like Amazon’s biased recruitment tool underscores the dangers of unregulated automation. Algorithmic bias, if unaddressed, can lead to systemic discrimination, legal repercussions, and a lack of workforce diversity. By integrating fairness principles, robust oversight mechanisms, and continuous bias monitoring, companies can (attempt to) ensure that AI-driven hiring tools foster equity and inclusivity rather than perpetuate existing biases.






Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, May 23). Machine Bias. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
Bogen, M. (2019, May 6). All the ways hiring algorithms can introduce bias. Harvard Business Review. https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias
Dastin, J. (2018, October 11). Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women. Reuters. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/
IBM. (2023, October 16). Shedding light on AI bias with real world examples. Ibm.com; IBM. https://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples
Johnson, T., & Johnson, N. (2023, May 18). Police Facial Recognition Technology Can’t Tell Black People Apart. Scientific American. https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/
A major question in Artificial Intelligence is “Can machines think?”. Last week, we discussed Turing’s “Imitation Game”, which proposed a test to measure machine intelligence, and various of its objections. The objection I felt was the strongest was the  Argument from Consciousness, which claims that machines are not comparable to brains, in the measure that they are unable to experience deep human emotions, namely grief, pride, arousal or sadness. My exploration of this argument focused on what “thinking” means and that the fact that we cannot distinguish the output of a machine and a human, it does not indicate anything about the similarity of the processes to achieve such output (which would be fundamental to the discussion of whether machines think). Part of the view I had was very well-aligned with the conclusions drawn from Searle’s “Chinese Room” argument. Searle’s thought experiment is based on the idea that a person can use symbol manipulation to process input in Chinese (which they do not understand) and provide output indistinguishable from a native speaker’s response, while still not understanding Chinese, or the content or purpose of the exchange. So, in Searle’s words “The only motivation for saying there must be a subsystem that understands Chinese is that I have a program and I can pass the Turing test; I can fool native Chinese speakers. But precisely one of the points at issue is the adequacy of the Turing test.”
The implementation of the classification algorithm “Naive Bayes” is a good object of analysis to clarify my stance on the topic of “understanding” by humans and machines. This program is computed in Python which is an imperative language and executes commands, changing the state of memory of the computer. The only perception of context and purpose is provided by the programmer who modularizes the code and chooses meaningful names (for him and future users) for variables and functions.
The program itself is a compilation of characters that follows syntax and semantics rules, and can be run. Daniel Dennett's “Intentional Systems Theory” supports the view that entities can be 'intentional systems' with goals, without true understanding. Applying this to Naive Bayes in sentiment analysis, it suggests that while the algorithm can classify emotions accurately, it doesn’t truly understand these emotions. It simply processes data to meet predefined goals, lacking any real comprehension or experience of the underlying sentiments. Therefore, I find the idea that this program has understanding of the sentiment of the tweets it processes ungraspable. Me, on the other hand, surely have AN understanding of the emotion of these tweets (as much as any person can be certain about someone else’s intention or emotion). My interpretation of communication (in written, but also spoken form) comes from years of attaching words and expressions to experience, emotion and culture. And therefore I am capable of empathy for shared situations, and I have access to analysis tools like sarcasm, humor, context and others (e.g. this food is to die for, this song is horribly underrated).
The next two questions presented focus on bridging the gap between the program and the human. Firstly, if I was asked to use someone else’s standards/tools, I would most likely use a program (humorous), but I will first address this question considering that this is not an option. I would attempt to systematically analyse many classifications and create a rule for myself on the classification of tweets with common themes, expressions, uses of humor, sarcasm, etc. Realistically, this is data that I understand, and therefore I would probably use my intuition as a baseline and adjust according to the systematic discrepancies I found in the classification. In this sense, I would argue that I would have an understanding of the rules/trends I found in the labeling of sentiment of tweets in this data set. If the condition was slightly different and I was blindly following rules in contextless data instances, then I too would have no understanding (like a calculus student mindlessly applying formulas in math). In the next scenario, I am in fact the programmer of a Naive Bayes sentiment analysis program. The combined system formed by me and the program could potentially be able to produce understanding of the labeled sentiment of tweets, where the emphasis should be placed on the labeled sentiment. If I hadn’t really grasped the working of this algorithm and how the labeled sentiment is computed, then some new understanding could in fact be produced. But outside of that, only a value (0 for negative and 4 for positive) or, possibly, dictionary can be outputted which have no inherent meaning outside of the understanding I, as the human programmer, attribute to it.
All in all, I consider that studying Searle’s “Chinese Room” argument was a very fruitful task for me, as I had the opportunity to further challenge and support some ideas/objections I developed during the previous week. I believe that computation and understanding are drastically different concepts. With each week, I feel that my understanding (... understanding…) of the philosophical discussion and implications of AI is more comprehensive.